{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "897ac6be",
   "metadata": {},
   "source": [
    "### Query Enhancement ‚Äì Query Expansion Techniques\n",
    "\n",
    "In a RAG pipeline, the quality of the query sent to the retriever determines how good the retrieved context is ‚Äî and therefore, how accurate the LLM‚Äôs final answer will be.\n",
    "\n",
    "That‚Äôs where Query Expansion / Enhancement comes in.\n",
    "\n",
    "#### üéØ What is Query Enhancement?\n",
    "Query enhancement refers to techniques used to improve or reformulate the user query to retrieve better, more relevant documents from the knowledge base.\n",
    "It is especially useful when:\n",
    "\n",
    "- The original query is short, ambiguous, or under-specified\n",
    "- You want to broaden the scope to catch synonyms, related phrases, or spelling variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88b2caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_openai import ChatOpenAI  # or ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e814ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Load and split the dataset\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "raw_docs =  loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33717603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000025E5D3CD550>, search_type='mmr', search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2: Vector Store\n",
    "\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore=FAISS.from_documents(chunks,embedding_model)\n",
    "\n",
    "## step 3:MMR Retriever\n",
    "retriever=vectorstore.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":5})\n",
    "retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec3e52f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(profile={'max_input_tokens': 200000, 'max_output_tokens': 100000, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x0000025E5EFF8440>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000025E5EFF8EC0>, root_client=<openai.OpenAI object at 0x0000025E5EC31550>, root_async_client=<openai.AsyncOpenAI object at 0x0000025E5EFF8C20>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## step 4 : LLM and Prompt\n",
    "from langchain.chat_models import init_chat_model\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(\"openai:o4-mini\")\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a1fced2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\\n\\nOriginal query: \"{query}\"\\n\\nExpanded query:\\n')\n",
       "| ChatOpenAI(profile={'max_input_tokens': 200000, 'max_output_tokens': 100000, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x0000025E5EFF8440>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000025E5EFF8EC0>, root_client=<openai.OpenAI object at 0x0000025E5EC31550>, root_async_client=<openai.AsyncOpenAI object at 0x0000025E5EFF8C20>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query expansion\n",
    "query_expansion_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\n",
    "\n",
    "Original query: \"{query}\"\n",
    "\n",
    "Expanded query:\n",
    "\"\"\")\n",
    "\n",
    "query_expansion_chain=query_expansion_prompt| llm | StrOutputParser()\n",
    "query_expansion_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f2a675b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(LangChain AND (memory OR ‚Äúmemory management‚Äù OR ‚Äústate management‚Äù OR ‚Äúsession memory‚Äù OR ‚Äúbuffer memory‚Äù OR ‚Äúsemantic memory‚Äù OR ‚Äúsummary memory‚Äù OR ‚Äúconversation memory‚Äù OR ‚Äúchat context‚Äù OR ‚Äúconversation history‚Äù OR ‚Äúcontext tracking‚Äù OR ‚Äústate persistence‚Äù OR ‚Äúcontextual embeddings‚Äù OR ‚Äúvector memory‚Äù)) AND (retriever OR ‚Äúvector store‚Äù OR embeddings OR FAISS OR Pinecone OR Milvus OR Weaviate OR Chroma OR Redis OR ‚ÄúMongoDB‚Äù OR ‚ÄúCassandra‚Äù)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_expansion_chain.invoke({\"query\": \"LangChain memory\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4116e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG answering prompt\n",
    "answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm=llm,prompt=answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b4c47a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline with query expansion\n",
    "rag_pipeline = (\n",
    "    RunnableMap({\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"context\": lambda x: retriever.invoke(query_expansion_chain.invoke({\"query\": x[\"input\"]}))\n",
    "    })\n",
    "    | document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0663c0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query:  \n",
      "‚ÄúWhat types of memory does LangChain support‚Äîincluding all its memory modules and capabilities for storing and retrieving context in LLM-based agents? For example:  \n",
      "‚Ä¢ Short-term ‚Äòbuffer‚Äô memory (ConversationBufferMemory)  \n",
      "‚Ä¢ Long-term or semantic memory (ConversationSummaryMemory, EntityMemory)  \n",
      "‚Ä¢ Vector-store memory (VectorStoreMemory, VectorStoreRetrieverMemory) using FAISS, Chroma, Weaviate, Redis, etc.  \n",
      "‚Ä¢ CombinedMemory and custom memory chains  \n",
      "Also consider synonyms and related terms such as cache, state persistence, context window, external memory, RAG (retrieval-augmented generation), memory API, conversation history, knowledge retrieval.‚Äù\n",
      "‚úÖ Answer:\n",
      " LangChain today ships at least two core memory implementations:\n",
      "\n",
      "‚Ä¢ ConversationBufferMemory ‚Äì keeps a full transcript of the back-and-forth in memory.  \n",
      "‚Ä¢ ConversationSummaryMemory ‚Äì compacts older turns into a running summary so you stay within LLM token limits.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"What types of memory does LangChain support?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"‚úÖ Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6e58191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query:\n",
      "\n",
      "(\n",
      "  ‚ÄúCrewAI Agents‚Äù OR \n",
      "  ‚ÄúCrew AI Agents‚Äù OR \n",
      "  ‚ÄúAI-driven crew agents‚Äù OR \n",
      "  ‚Äúdigital crew assistants‚Äù OR \n",
      "  ‚Äúvirtual crew assistants‚Äù OR \n",
      "  ‚Äúautonomous crew scheduling agents‚Äù OR \n",
      "  ‚Äúintelligent staffing bots‚Äù OR \n",
      "  ‚Äúworkforce automation agents‚Äù OR \n",
      "  ‚Äúteam coordination agents‚Äù OR \n",
      "  ‚Äúmulti-agent system‚Äù OR \n",
      "  ‚Äúagent-based modeling‚Äù OR \n",
      "  ‚Äúautonomous agent framework‚Äù\n",
      ")\n",
      "AND\n",
      "(\n",
      "  platform OR framework OR API OR architecture OR documentation OR ‚Äúbest practices‚Äù OR tutorial OR ‚Äúcase study‚Äù\n",
      ")\n",
      "AND\n",
      "(\n",
      "  ‚Äúcrew management‚Äù OR scheduling OR ‚Äúresource allocation‚Äù OR ‚Äútask assignment‚Äù OR ‚Äúreal-time collaboration‚Äù OR ‚Äúhuman-AI collaboration‚Äù OR ‚Äúreinforcement learning‚Äù OR ‚Äúdeep learning‚Äù OR ‚Äúnatural language processing‚Äù OR chatbot\n",
      ")\n",
      "‚úÖ Answer:\n",
      " CrewAI agents are autonomous ‚Äúcrew members‚Äù that collaborate in structured workflows to solve complex tasks. Key characteristics include:  \n",
      "1. Defined Roles ‚Äì each agent is given a clear role (e.g. researcher, planner, executor) so responsibilities don‚Äôt overlap and hand-offs are clean.  \n",
      "2. Purpose & Goal ‚Äì every agent knows its high-level purpose and the specific goal it must achieve toward the overall crew objective.  \n",
      "3. Toolset ‚Äì agents are provisioned with the API calls, data sources, or code libraries they need, and they use those tools to carry out their tasks.  \n",
      "4. Semi-independence ‚Äì they reason and act on their own within the crew‚Äôs structure, but they coordinate results and status back to the team.  \n",
      "5. Scalability ‚Äì you can add more agents for horizontal scaling or deepen each agent‚Äôs reasoning chain for vertical scaling, allowing the system to tackle larger or more intricate problems.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"CrewAI Agents?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"‚úÖ Answer:\\n\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learning_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
