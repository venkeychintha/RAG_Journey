{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f169352",
   "metadata": {},
   "source": [
    "### ðŸ§  What is Query Decomposition?\n",
    "Query decomposition is the process of taking a complex, multi-part question and breaking it into simpler, atomic sub-questions that can each be retrieved and answered individually.\n",
    "\n",
    "#### âœ… Why Use Query Decomposition?\n",
    "\n",
    "- Complex queries often involve multiple concepts\n",
    "\n",
    "- LLMs or retrievers may miss parts of the original question\n",
    "\n",
    "- It enables multi-hop reasoning (answering in steps)\n",
    "\n",
    "- Allows parallelism (especially in multi-agent frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f230d79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_openai import ChatOpenAI  # or ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d02f45c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and embed the document\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"lambda_mult\": 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce62db88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000001DE74A8B4D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001DE74CC4190>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(model=\"groq:llama-3.1-8b-instant\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9806282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query decomposition\n",
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Sub-questions:\n",
    "\"\"\")\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdde4ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decomposition_question=decomposition_chain.invoke({\"question\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43ab160b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To facilitate better document retrieval, I can decompose the complex question into the following sub-questions:\n",
      "\n",
      "1. **What is LangChain's architecture and how does it utilize memory in conversational AI?**\n",
      "\n",
      "   This sub-question aims to understand the memory component of LangChain and how it's designed to handle conversational tasks.\n",
      "\n",
      "2. **How do agents interact with the memory component in LangChain?**\n",
      "\n",
      "   This sub-question delves into the specifics of how agents in LangChain engage with the memory, highlighting their role in the system.\n",
      "\n",
      "3. **What is CrewAI's architecture and how does it utilize memory and agents in conversational AI?**\n",
      "\n",
      "   This sub-question provides context on CrewAI by understanding its overall architecture and how it employs memory and agents in conversational AI tasks.\n",
      "\n",
      "4. **How does LangChain's use of memory and agents compare to CrewAI in terms of performance, scalability, or other relevant metrics?**\n",
      "\n",
      "   This sub-question aims to identify key differences or similarities between LangChain and CrewAI in terms of their memory and agent usage, highlighting any advantages or disadvantages of each approach.\n"
     ]
    }
   ],
   "source": [
    "print(decomposition_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8750ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: QA chain per sub-question\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b2ecc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline logic\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    # Decompose the query\n",
    "    sub_qs_text = decomposition_chain.invoke({\"question\": user_query})\n",
    "    sub_questions = [q.strip(\"-â€¢1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()]\n",
    "    \n",
    "    results = []\n",
    "    for subq in sub_questions:\n",
    "        docs = retriever.invoke(subq)\n",
    "        result = qa_chain.invoke({\"input\": subq, \"context\": docs})\n",
    "        results.append(f\"Q: {subq}\\nA: {result}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a71ad0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final Answer:\n",
      "\n",
      "Q: Here are 4 sub-questions that can help decompose the complex question:\n",
      "A: It seems like you've provided a context related to LangChain and CrewAI, but you haven't asked a question yet. However, I can help you break down the complex topic into the 4 sub-questions you mentioned. Here are the sub-questions:\n",
      "\n",
      "1. Can you explain the composition and reusability of chains in LangChain using standard patterns like Stuff, Map-Reduce, and Refine?\n",
      "2. What types of workflows benefit from specialization and collaboration in CrewAI?\n",
      "3. What are some common applications of LangChain agents, such as web search, calculators, and code execution environments?\n",
      "4. How does the planner-executor model in LangChain agents enable dynamic decision-making, branching logic, and context-aware memory use across steps?\n",
      "\n",
      "Q: **What is LangChain, and what are its key components (memory and agents)?**\n",
      "A: Based on the provided context, I can answer your question:\n",
      "\n",
      "**What is LangChain?**\n",
      "LangChain is an open-source framework designed for developing applications powered by large language models (LLMs). It simplifies the process of building, managing, and scaling complex chains of thought by abstracting prompt management, retrieval, memory, and agent orchestration.\n",
      "\n",
      "**Key components (memory and agents):**\n",
      "\n",
      "1. **Memory:** LangChain offers memory capabilities that allow for context-aware memory use across steps. This means that agents can retain information from previous steps and use it to inform their decisions in subsequent steps.\n",
      "2. **Agents:** LangChain agents operate using a planner-executor model, where the agent plans out a sequence of tool invocations to achieve a goal. Agents use LLMs to reason about which tool to call, what input to provide, and how to process the output, enabling dynamic decision-making, branching logic, and integration with various tools like web search, calculators, and code execution.\n",
      "\n",
      "Q: This sub-question aims to understand the fundamental architecture and concepts of LangChain\n",
      "A: Based on the provided context, the fundamental concepts and architecture of LangChain can be summarized as follows:\n",
      "\n",
      "1. **Chains**: At the heart of LangChain lies the concept of chains, which are sequences of calls to Large Language Models (LLMs) and other tools.\n",
      "2. **Types of Chains**: Chains can be simple (single prompt fed to an LLM) or complex (involving multiple conditionally executed steps).\n",
      "3. **Composability**: LangChain makes it easy to compose and reuse chains using standard libraries and tools.\n",
      "4. **Tool Integration**: Chains can involve calls to LLMs and other tools, allowing for a wide range of applications and use cases.\n",
      "5. **Conditional Execution**: Complex chains can involve conditionally executed steps, enabling more sophisticated and dynamic behavior.\n",
      "\n",
      "Overall, the architecture of LangChain appears to be centered around the concept of chains, which can be composed and reused to build complex applications powered by LLMs and other tools.\n",
      "\n",
      "Q: **How does LangChain utilize memory in its framework?**\n",
      "A: LangChain utilizes memory in its framework through memory modules such as ConversationBufferMemory and ConversationSummaryMemory. These modules enable the Large Language Model (LLM) to:\n",
      "\n",
      "1. Maintain awareness of previous conversation turns (ConversationBufferMemory)\n",
      "2. Summarize long interactions to fit within token limits (ConversationSummaryMemory)\n",
      "\n",
      "This abstraction of prompt management, retrieval, memory, and agent orchestration simplifies the process of building, managing, and scaling complex chains of thought in applications powered by LLMs.\n",
      "\n",
      "Q: This sub-question focuses on LangChain's memory component, its functionality, and its applications\n",
      "A: Based on the given context, here are the answers to the question:\n",
      "\n",
      "1. **What is LangChain's memory component?**\n",
      "LangChain offers memory modules like ConversationBufferMemory and ConversationSummaryMemory. These modules allow the LLM (Large Language Model) to maintain awareness of previous conversation turns or summarize long interactions to fit within token limits.\n",
      "\n",
      "2. **What is the functionality of LangChain's memory components?**\n",
      "The functionality of these memory components is to:\n",
      "- Maintain awareness of previous conversation turns\n",
      "- Summarize long interactions to fit within token limits\n",
      "\n",
      "3. **What are the applications of LangChain's memory components?**\n",
      "The memory components can be applied in various ways, including:\n",
      "- Maintaining context and awareness in conversations\n",
      "- Summarizing long interactions to avoid exceeding token limits\n",
      "- Building scalable and maintainable LLM applications\n",
      "\n",
      "Q: **What is the role of agents in LangChain, and how do they interact with memory?**\n",
      "A: According to the context, LangChain agents use Large Language Models (LLMs) to reason about which tools to call, what input to provide, and how to process the output. However, the specific role of agents in LangChain, and their interaction with memory, can be summarized as follows:\n",
      "\n",
      "**Role of agents in LangChain:** \n",
      "LangChain agents are responsible for planning out a sequence of tool invocations to achieve a goal. They can execute multi-step tasks, integrating with various tools such as web search, calculators, and code execution.\n",
      "\n",
      "**Interaction with memory:** \n",
      "LangChain agents can use context-aware memory across steps, which implies that they can recall and utilize information from previous steps to inform their decisions in subsequent steps. This allows for dynamic decision-making and branching logic, making the agents more adaptable and effective in achieving their goals.\n",
      "\n",
      "Q: This sub-question delves into the agent component of LangChain, its functionality, and its relationship with memory\n",
      "A: Based on the provided context, here are the key points related to the agent component of LangChain and its relationship with memory:\n",
      "\n",
      "1. **Agent Functionality**: LangChain agents use Large Language Models (LLMs) to reason about which tool to call, what input to provide, and how to process the output.\n",
      "2. **Agent Type**: LangChain agents operate using a planner-executor model, where the agent plans out a sequence of tool invocations to achieve a goal.\n",
      "3. **Dynamic Decision-Making**: LangChain agents support dynamic decision-making, which allows them to make decisions based on the output of previous steps.\n",
      "4. **Branching Logic**: LangChain agents can use branching logic, which enables them to execute different paths based on specific conditions.\n",
      "5. **Context-Aware Memory Use**: LangChain agents can use context-aware memory, which allows them to retain information and use it across multiple steps.\n",
      "\n",
      "In summary, LangChain agents are capable of complex decision-making, branching logic, and context-aware memory use, making them powerful tools for multi-step tasks.\n",
      "\n",
      "Q: **How does CrewAI compare to LangChain in terms of memory and agent architecture?**\n",
      "A: Unfortunately, the provided context does not provide any information about how CrewAI compares to LangChain in terms of memory and agent architecture. The context only mentions that CrewAI is compatible with LangChain agents and tools and that CrewAI manages role-based collaboration, while LangChain handles retrieval and tool wrapping. It does not provide any comparison of their memory or agent architecture.\n",
      "\n",
      "Q: This sub-question compares the two frameworks, focusing on their similarities and differences in memory and agent usage\n",
      "A: There is no information provided in the context about the two frameworks, their memory usage, or their differences/similarities in agent usage. The context only discusses the features and benefits of a single framework, specifically its support for full traceability of agent decisions and interactions, and its use of agent context-sharing.\n",
      "\n",
      "Q: These sub-questions can help guide document retrieval and provide a clearer understanding of the original complex question\n",
      "A: Based on the context provided, the sub-questions that can help guide document retrieval and provide a clearer understanding of the original complex question seem to be related to semantic search and Retrieval-Augmented Generation (RAG). \n",
      "\n",
      "Here's a possible set of sub-questions that can guide document retrieval:\n",
      "\n",
      "1. What are the key concepts and entities mentioned in the original question?\n",
      "2. What are the specific domains or topics relevant to the question?\n",
      "3. Are there any specific documents, authors, or sources that are known to be relevant to the topic?\n",
      "4. Are there any specific keywords, phrases, or synonyms that can be used to search for relevant documents?\n",
      "5. Are there any specific document types or formats that are relevant to the search (e.g. academic papers, news articles, etc.)?\n",
      "\n",
      "These sub-questions can help refine the search query and guide the retrieval of relevant documents, ultimately providing a clearer understanding of the original complex question.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"âœ… Final Answer:\\n\")\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learning_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
